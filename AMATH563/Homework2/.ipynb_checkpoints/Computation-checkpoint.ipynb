{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2ebfdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MultivariateStats, CSV, DataFrames, KernelFunctions, LinearAlgebra, Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aaa2d1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1f3ba",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we apply three different kernel methods in order to classify handwritten digits from the MNIST data set. To do this, we first apply principal component analysis to reduce the dimension of our training data to 154, preserving 95% of the variance. We then build classifiers via a kernel ridge regression with three different choices of kernels: linear, polynomial, and RBF (Gaussian). We do this for the pairs of digits $(1,9)$, $(3,8)$, $(1,7)$, and $(5,2)$. We find that the pairs $(3,8)$ and $(5,2)$ are the hardest to classify correctly and that in all cases, polynomial and RBF kernels when tuned well outperform the linear kernel by a large margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57fea9",
   "metadata": {},
   "source": [
    "Let's start by reading the training and test data in, converting them to matrices, and separating the labels from the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb561ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = Matrix(CSV.read(\"mnist_train.csv\", DataFrame))\n",
    "training_data = t_data[:,2:end]\n",
    "training_labels = t_data[:,1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfcd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Matrix(CSV.read(\"mnist_test.csv\", DataFrame))\n",
    "testing_data = test_data[:,2:end]\n",
    "testing_labels = test_data[:,1];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e6c41",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e2253",
   "metadata": {},
   "source": [
    "We begin by using principal component analysis (PCA) to reduce the dimension of our input but preserve 95% of the variance. In doing this, we use the Julia package MultivariateStats.jl. This package is pretty black box, so we begin with a brief explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4500a",
   "metadata": {},
   "source": [
    "PCA can be understood through either a covariance matrix decomposition or the SVD; however, since the dimension is smaller than the number of observations, MultivariateStats defaults to the former. The broad idea of this approach is that a covariance matrix of our data is constructed, and an eigendecomposition of this matrix is computed. We then select the eigenvectors in decreasing order of the size of their eigenvalues, stopping when at least 95% of the variance is preserved. We find that this requires 154 prinicpal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b7962d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of modes needed: 154\n"
     ]
    }
   ],
   "source": [
    "M = fit(PCA,training_data';pratio=0.95);\n",
    "@printf(\"Number of modes needed: %i\\n\",size(M,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f15a9e",
   "metadata": {},
   "source": [
    "Now, we project both our training data and testing data onto the span of the selected eigenvectors. This is what we will use as our data input for the rest of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa522caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte = predict(M, training_data')' |> Array;\n",
    "Yte = predict(M, testing_data')' |> Array;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf852f4",
   "metadata": {},
   "source": [
    "## Kernel regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa787416",
   "metadata": {},
   "source": [
    "For ease of implimentation, we focus solely on kernel ridge regression. The general framework for this is outlined in the course notes from lecture 8. Assuming $\\mathcal{H}$ is an RKHS with kernel $K$, we look to solve \n",
    "$$\\min_{f\\in\\mathcal{H}}=\\sum_{j=1}^n|f(x_j)-y_j|^2+\\frac{\\lambda}{2}\\|f\\|^2.$$ \n",
    "The representer theorem allows us to write this as the problem \n",
    "$$\\min_{z\\in\\mathbb{R}^n}\\|z-y\\|^2+\\frac{\\lambda}{2}z^TK(X,X)^{-1}z.$$\n",
    "We can write the optimal solution in a slight different from than expressed in class as\n",
    "$$z^*=K(X^*,X)(K(X,X)+\\lambda I)^{-1}y,$$\n",
    "which we impliment in the following function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b848e",
   "metadata": {},
   "source": [
    "In what follows, we'll try three choices of the kernel $K$ which we obtain from the KernelFunctions.jl package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c8d8ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "function kernel_ridge_regression(k, X, y, Xstar, lambda)\n",
    "    K = kernelmatrix(k, X)\n",
    "    kstar = kernelmatrix(k, Xstar, X)\n",
    "    return kstar * ((K + lambda * I) \\ y)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfc049",
   "metadata": {},
   "source": [
    "Now, we write functions which given an output, check what percentage of observations were classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "af8f2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(x) = x>0.5 ? 1. : -1.;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "83f1f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "function pct_classified(output,labels)\n",
    "    class = classify.(output)\n",
    "    incorrect = sum(abs.(classify.(output)-labels))/2\n",
    "    @printf(\"percent classified correctly: %f%%\\n\",(length(labels)-incorrect)/length(labels)*100)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243fc25",
   "metadata": {},
   "source": [
    "Let's collect the data corresponding to the digits 1 and 9 and attempt to classify with kernel ridge regression with linear, polynomial, and RBF kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23d265d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = findall(training_labels.==1)\n",
    "index9 = findall(training_labels.==9)\n",
    "X₁ = Xte[[index1; index9],:]\n",
    "y₁ = [ones(size(index1)); -ones(size(index9))];\n",
    "index1t = findall(testing_labels.==1)\n",
    "index9t = findall(testing_labels.==9)\n",
    "Xstar₁ = Yte[[index1t; index9t],:];\n",
    "ystar₁ = [ones(size(index1t)); -ones(size(index9t))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928b9bd",
   "metadata": {},
   "source": [
    "We first try a linear kernel $$K(x,x')=x^Tx'.$$ KernelFunctions allows us to add a centering term as well but uses zero by default. Choosing $\\lambda=0.1$ seems to give decent results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d86f9b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 99.067164%\n"
     ]
    }
   ],
   "source": [
    "output1 = kernel_ridge_regression(LinearKernel(),X₁',y₁,Xstar₁',0.1);\n",
    "pct_classified(output1,ystar₁)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e0d86",
   "metadata": {},
   "source": [
    "Now, we use a polynomial kernel $$K(x,x')=(x^Tx'+c)^\\alpha,$$ where $\\alpha\\in\\mathbb{N}$. By default, KernelFunctions uses $\\alpha=2$ and $c=0$, but using a quartic polynomial ($\\alpha=4$) seems to give better results here. $c=0$ and $\\lambda=0.1$ seem to still work here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3e8be361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 99.720149%\n"
     ]
    }
   ],
   "source": [
    "output2 = kernel_ridge_regression(PolynomialKernel(degree=4),X₁',y₁,Xstar₁',0.1);\n",
    "pct_classified(output2,ystar₁)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baffd48",
   "metadata": {},
   "source": [
    "Finally, we try an RBF kernel $$K(x,x')=\\exp\\left(-\\frac{\\|x-x'\\|_2^2}{2\\sigma^2}\\right).$$ KernelFunctions only allows for $\\sigma=1$, so we have to rescale our data to adjust this parameter. We find that $\\sigma=1000$, $\\lambda=0.1$ gives good results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "068aa5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 99.813433%\n"
     ]
    }
   ],
   "source": [
    "output3 = kernel_ridge_regression(RBFKernel(),X₁'/1000,y₁,Xstar₁'/1000, 0.1);\n",
    "pct_classified(output3,ystar₁)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7fe63",
   "metadata": {},
   "source": [
    "For this case, it seems that all three kernels perform well, but polynomial and RBF perform a bit better than linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034240aa",
   "metadata": {},
   "source": [
    "Now, let's repeat this experiment for 3s and 8s, which should be a bit more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ede0cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "index3 = findall(training_labels.==3)\n",
    "index8 = findall(training_labels.==8)\n",
    "X₂ = Xte[[index3; index8],:]\n",
    "y₂ = [ones(size(index3)); -ones(size(index8))];\n",
    "index3t = findall(testing_labels.==3)\n",
    "index8t = findall(testing_labels.==8)\n",
    "Xstar₂ = Yte[[index3t; index8t],:];\n",
    "ystar₂ = [ones(size(index3t)); -ones(size(index8t))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f787cf",
   "metadata": {},
   "source": [
    "We see that the linear kernel performs significantly worse than in the previous case, but we can only tinker with the regularization term which doesn't affect too much. $\\lambda=0.1$ seems to be about optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0e1a1515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 90.473790%\n"
     ]
    }
   ],
   "source": [
    "output1 = kernel_ridge_regression(LinearKernel(),X₂',y₂,Xstar₂',0.1);\n",
    "pct_classified(output1,ystar₂)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d67c0",
   "metadata": {},
   "source": [
    "Reducing the degree of our polynomial (using $\\alpha=3$ instead of $\\alpha=4$) produces better results, but we still aren't nearly as accurate as the previous case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e2a2ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 98.135081%\n"
     ]
    }
   ],
   "source": [
    "output2 = kernel_ridge_regression(PolynomialKernel(degree=3),X₂',y₂,Xstar₂',0.1);\n",
    "pct_classified(output2,ystar₂)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63606c4",
   "metadata": {},
   "source": [
    "As before, RBF performs just slightly better than polynomial but much worse than in the previous case. The same hyperparameters ($\\sigma=1000$, $\\lambda=0.1$) still seem to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ec11f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 98.538306%\n"
     ]
    }
   ],
   "source": [
    "output3 = kernel_ridge_regression(RBFKernel(),X₂'/1000,y₂,Xstar₂'/1000,0.1);\n",
    "pct_classified(output3,ystar₂)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897a27c",
   "metadata": {},
   "source": [
    "Now, we try to classify 1s and 7s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ea28b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "index7 = findall(training_labels.==7)\n",
    "X₃ = Xte[[index1; index7],:]\n",
    "y₃ = [ones(size(index1)); -ones(size(index7))];\n",
    "index7t = findall(testing_labels.==7)\n",
    "Xstar₃ = Yte[[index1t; index7t],:];\n",
    "ystar₃ = [ones(size(index1t)); -ones(size(index7t))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80f6c9",
   "metadata": {},
   "source": [
    "Linear does decently again, but is still not as accurate as we'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b6858ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 98.289413%\n"
     ]
    }
   ],
   "source": [
    "output1 = kernel_ridge_regression(LinearKernel(),X₃',y₃,Xstar₃',0.1);\n",
    "pct_classified(output1,ystar₃)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833e325",
   "metadata": {},
   "source": [
    "Now, a degree 4 polynomial seems like the optimal choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bc3ab276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 99.722607%\n"
     ]
    }
   ],
   "source": [
    "output2 = kernel_ridge_regression(PolynomialKernel(degree=4),X₃',y₃,Xstar₃',0.1);\n",
    "pct_classified(output2,ystar₃)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c9029",
   "metadata": {},
   "source": [
    "Choosing $\\sigma=1000$ still seems optimal. Interestingly, we get the same accuracy for RBF and polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "13c4993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 99.722607%\n"
     ]
    }
   ],
   "source": [
    "output3 = kernel_ridge_regression(RBFKernel(),X₃'/1000,y₃,Xstar₃'/1000,0.1);\n",
    "pct_classified(output3,ystar₃)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d42ce",
   "metadata": {},
   "source": [
    "Finally, we look to classify 5s and 2s. This should also be a bit more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7257858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index5 = findall(training_labels.==5)\n",
    "index2 = findall(training_labels.==2)\n",
    "X₄ = Xte[[index5; index2],:]\n",
    "y₄ = [ones(size(index5)); -ones(size(index2))];\n",
    "index5t = findall(testing_labels.==5)\n",
    "index2t = findall(testing_labels.==2)\n",
    "Xstar₄ = Yte[[index5t; index2t],:];\n",
    "ystar₄ = [ones(size(index5t)); -ones(size(index2t))];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a37111",
   "metadata": {},
   "source": [
    "The linear kernel once again does poorly, but there's not much that we can do to amend this other than use a different kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "191c2057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 93.139293%\n"
     ]
    }
   ],
   "source": [
    "output1 = kernel_ridge_regression(LinearKernel(),X₄',y₄,Xstar₄',0.1);\n",
    "pct_classified(output1,ystar₄)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ee705",
   "metadata": {},
   "source": [
    "Interestingly, the degree 3 polynomial seems to be the right choice here. It seems that $\\alpha=3$ is optimal when the digits are difficult to distinguish between while $\\alpha=4$ works better when the digits are easier to tell apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dc1011e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 99.272349%\n"
     ]
    }
   ],
   "source": [
    "output2 = kernel_ridge_regression(PolynomialKernel(degree=3),X₄',y₄,Xstar₄',0.1);\n",
    "pct_classified(output2,ystar₄)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573d23c",
   "metadata": {},
   "source": [
    "Once again, $\\sigma=1000$ seems to be the optimal choice for RBF, and it performs comparably to the polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4d4fda76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent classified correctly: 99.324324%\n"
     ]
    }
   ],
   "source": [
    "output3 = kernel_ridge_regression(RBFKernel(),X₄'/1000,y₄,Xstar₄'/1000,0.1);\n",
    "pct_classified(output3,ystar₄)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27197f6c",
   "metadata": {},
   "source": [
    "Overall, we find that when well-tuned, the polynomial and RBF kernels perform fairly well, while the linear kernel performs much worse. This suggests that threshold which divides our data is simply nonlinear, so nonlinear functions will never be able to capture it well. A linear kernel is really just equivalent to OLS, so this says that in order to fit MNIST accurately, we need to do something fancier than linear regression; however, this is a simple enough data set that just adding some basic nonlinearities improves our classification substantially. Hence, polynomial and RBF kernels perform quite well, even when distinguishing between similar-looking digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d173ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
